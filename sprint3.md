# Sprint 3 - Worksheet

</br>

## 1. Load Testing

### Load Testing Environment

### Test Report

### Interesting Bottleneck

### Were non-functional requirements met?

</br>
</br>

## 2. Security Analysis

### Chosen Tool & Report

### 5 Decected Problems

### Critical/High Vulnerabilities and Fixes

</br>
</br>

## 3. Continuous Integration & Deployment (CI/CD)

### CI Execution

### CD Execution

</br>
</br>

## 4. Reflections

### Design Changes
As mentioned in both our presentations, we tried to do a bulk of the architecture planning and data modeling within the first 2 sprints. That is, we came to consensus on a majority of the project's scope and features within Sprint 0, alongside a detailed data model. This model (which we incorrectly called our architecture model at first) had many positives, like how the backend schema and domain objects were clearly outlined, streamlining development. However, we would change a great many things about this process and by extension, the design of the project since we became too dependant on accepting this model as a single source of truth. It follows that gaps within this "inclusive plan" kept appearing the further the project went on. In short, we would still create this data model in our next project, but treat it as "loosely defined guidelines, not the rulebook itself."

### Project Setup Changes
We think that the freedom this course gives students in choosing everything related to their project (if the source code lives in an accessible repo) is a bit of a double-edged sword. The course notes provide many examples to point students in the right direction when choosing a tech stack, which we believe is fine; this invites group or independent research. Therefore, students naturally begin to look at modern resources (such as Docker, automated docs like Swagger, etc.) which is an essential skill to learn, given this is a 4th year course and most of us are ready to graduate by end-of-year. Conversely, this freedom begins to show its limitations once the worksheets start stating the requirements. Some of the sprint requirements felt like too much, overcomplicated, or unnecessary, especially near the end of the term when this course's deliverables began to overtake what we could realistically output. Finally, we think the sprint feedback needs to be reworked, as it represents everything mentioned above. The course encompasses so many topics and parts of the software development life cycle, and so the sprint feedback should have very well-defined standards. For example, our early sprints' feedback commended us on our documentation, app styling standards, and data modeling practices; the next sprint went on to say that those elements wasted too much time and were not part of what was being graded in this course.

### Individual AI / External Resource Reflections

#### Alejandro
> I am a Co-op student who has taken most of the courses which contain group projects *after* my final work term. My work terms had my working on CI/CD, Data Modeling, and Application Architecture. As such, all the previously mentioned group projects had me working on a high-level, app-wide scope, with contributions to the actual code when needed.  
>   
> It follows that, given our permissible usage of AI in this project, I used AI from the start to plan out everything related to the architecture and data model. The queries usually started with an explanation of the project and a definition of the domain objects (Vehicles, Cards, Trades, etc.), alongside a request on how to best setup the data model to maximize parallel development. I used the MVVM method from the start because I have experience with Client - API/Controllers - Database layering systems. Responses were usually very accurate and helped me brainstorm the high-level decisions, but they did attempt to overcomplicate things if I kept a chat going for too long. Additionally, I helped define all the styles and design of the app inside Figma, which was then ported over to React-friendly files, such as .css files and classes. As a final note, since we mentioned how the data model itself had major, unknown gaps in it from the start, the AI I used also did not catch these issues. Whether that was an issue in the lack of detail in my prompts, or the AI simply did what it was told without worry about potential gaps, we will never know; use AI as a supplementary tool, not a strict guide!

#### Ian
> 

### Subhash 
>

#### Ansh
> 

#### Vansh
> 

### Jotham
>
